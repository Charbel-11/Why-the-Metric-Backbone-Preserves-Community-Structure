{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZU4JOmxwt1w"
      },
      "outputs": [],
      "source": [
        "!pip install scipy==1.8.1\n",
        "import scipy\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOWnfCHsZKKc"
      },
      "outputs": [],
      "source": [
        "!echo \"deb http://downloads.skewed.de/apt jammy main\" >> /etc/apt/sources.list\n",
        "!apt-key adv --keyserver keyserver.ubuntu.com --recv-key 612DEFB798507F25\n",
        "!apt-get update\n",
        "!apt-get install python3-graph-tool python3-matplotlib python3-cairo\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gb7tATtBZR8F"
      },
      "outputs": [],
      "source": [
        "!apt purge python3-cairo\n",
        "!apt install libcairo2-dev pkg-config python3-dev\n",
        "!pip install --force-reinstall pycairo\n",
        "!pip install zstandard\n",
        "!pip install rustworkx\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPxO94iKXHdB"
      },
      "outputs": [],
      "source": [
        "!pip install infomap\n",
        "!pip install igraph\n",
        "!pip install pygsp\n",
        "!pip install graphlearning\n",
        "!pip install ucimlrepo\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcDzfGXIeuf2",
        "outputId": "6896a1e3-dd90-408b-f9c5-8de550bdee37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uRqhPfcy7yo"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/GSMB')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hu7OdaBhNCK-",
        "outputId": "b210d0c3-777f-47c5-aa10-e08a408a46d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "amazon.txt\t\t\t D_Amazon\t   helper_plots.py     Results\n",
            "B_Amazon\t\t\t data\t\t   knn_data\t       S_Amazon\n",
            "clustering.py\t\t\t Datasets\t   metric_backbone.py  spectral_clustering\n",
            "community_experiments_plots.py\t datasets.py\t   metrics.py\t       T_Amazon\n",
            "community_experiments_tables.py  graph_builder.py  __pycache__\t       TSC.py\n"
          ]
        }
      ],
      "source": [
        "!ls drive/MyDrive/GSMB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8uq9GrCzttH"
      },
      "outputs": [],
      "source": [
        "def get_Amazon_meta():\n",
        "    f = open(\"drive/MyDrive/GSMB/amazon.txt\", 'r')\n",
        "    n = int(f.readline())\n",
        "    print(\"n\", n)\n",
        "\n",
        "    idxToLabel = {}\n",
        "    for i in range(n):\n",
        "        line = f.readline().split(\" \")\n",
        "        x = int(float(line[0]))\n",
        "        label = int(float(line[1]))\n",
        "        idxToLabel[x] = label\n",
        "\n",
        "    f.close()\n",
        "    return idxToLabel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYLC0J5LZfL5"
      },
      "outputs": [],
      "source": [
        "import graph_tool.all as gt\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import time\n",
        "\n",
        "def get_Bayesian_partition(D, weight='proximity'):\n",
        "    start = time.time()\n",
        "    Dweights = np.array([D[e[0]][e[1]][weight] for e in D.edges])\n",
        "    edges = np.array([e for e in D.edges])\n",
        "\n",
        "    g = gt.Graph(directed=False)\n",
        "    g.add_edge_list(edges)\n",
        "    ew = g.new_edge_property(\"double\")\n",
        "    ew.a = Dweights\n",
        "    g.ep['weight'] = ew\n",
        "\n",
        "    state = gt.minimize_blockmodel_dl(g, state_args=dict(recs=[g.ep.weight],\n",
        "                                                            rec_types=[\"real-exponential\"]))\n",
        "\n",
        "    blocks = state.get_blocks().get_array()\n",
        "    partition = {}\n",
        "    clusterTypes = set()\n",
        "    for i in range(len(blocks)):\n",
        "        partition[i] = blocks[i]\n",
        "        clusterTypes.add(partition[i])\n",
        "\n",
        "    #For isolated vertices\n",
        "    choices = list(clusterTypes)\n",
        "    for i in range(len(blocks), D.number_of_nodes()):\n",
        "        partition[i] = random.choice(choices)\n",
        "\n",
        "    end = time.time()\n",
        "    print(\"Bayesian Fit executed in %.3f s\" % (end-start))\n",
        "    print(\"Num partitions =\", len(clusterTypes))\n",
        "\n",
        "    partitions = {}\n",
        "    partitions['Bayesian'] = (partition, len(clusterTypes))\n",
        "    return partitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aop4itHWZ9ll",
        "outputId": "4ceed3e4-c1a3-4dcf-8bf9-d774447fba35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Built the distance graph in 2.559 s\n",
            "Built the metric backbone graph in 0.401 s\n",
            "Spielman sparisfier ran in 0.473 s\n",
            "Built the Spielman graph in 0.583 s\n",
            "Built the threshold graph in 0.034 s\n",
            "Bayesian Fit executed in 4.741 s\n",
            "Num partitions = 16\n",
            "Bayesian Fit executed in 0.940 s\n",
            "Num partitions = 10\n",
            "Bayesian Fit executed in 1.928 s\n",
            "Num partitions = 4\n",
            "Bayesian Fit executed in 0.811 s\n",
            "Num partitions = 6\n",
            "HI\n",
            "Built the distance graph in 7.762 s\n",
            "Built the metric backbone graph in 3.272 s\n",
            "Spielman sparisfier ran in 31.034 s\n",
            "Built the Spielman graph in 31.380 s\n",
            "Built the threshold graph in 0.074 s\n",
            "Bayesian Fit executed in 11.553 s\n",
            "Num partitions = 32\n",
            "Bayesian Fit executed in 2.902 s\n",
            "Num partitions = 24\n",
            "Bayesian Fit executed in 3.924 s\n",
            "Num partitions = 21\n",
            "Bayesian Fit executed in 3.152 s\n",
            "Num partitions = 19\n"
          ]
        }
      ],
      "source": [
        "from metrics import *\n",
        "from datasets import *\n",
        "from clustering import *\n",
        "from graph_builder import *\n",
        "import networkx as nx\n",
        "\n",
        "initialPath = '/content/drive/MyDrive/GSMB/'\n",
        "\n",
        "def get_partitions_similarity_ARI(p1, p2):\n",
        "    if p1 is None or p2 is None:\n",
        "        return -1\n",
        "\n",
        "    vals1 = []\n",
        "    vals2 = []\n",
        "    for i in p1.keys():\n",
        "        vals1.append(p1[i])\n",
        "        vals2.append(p2[i])\n",
        "    return sklearn.metrics.adjusted_rand_score(vals1, vals2)\n",
        "\n",
        "def get_similarities(f, type, partitions, partitions_D, partition_Meta=None):\n",
        "    similarity_D = {}\n",
        "    similarity_metaLabels = {}\n",
        "    for algo in partitions:\n",
        "        (partition, cluster) = partitions[algo]\n",
        "        (partitionD, clusterD) = partitions_D[algo]\n",
        "        similarity_D[algo] = get_partitions_similarity_ARI(partitionD, partition)\n",
        "        similarity_metaLabels[algo] = get_partitions_similarity_ARI(partition_Meta, partition)\n",
        "\n",
        "    if partition_Meta is not None:\n",
        "        for algo in similarity_metaLabels.keys():\n",
        "            f.write(type + \" Meta \" + algo + \" \" + str(similarity_metaLabels[algo]) + \"\\n\")\n",
        "    for algo in similarity_D.keys():\n",
        "        f.write(type + \" Original \" + algo + \" \" + str(similarity_D[algo]) + \"\\n\")\n",
        "\n",
        "\n",
        "def compute_similarities_real_datasets_Bayesian():\n",
        "    get_dataset_array = [get_high_school_dataset, get_primary_school_dataset, get_DBLP_dataset, get_Amazon_dataset]\n",
        "    title_array = [\"High_School\", \"Primary_School\", \"DBLP\", \"Amazon\"]\n",
        "    has_meta_array = [True, True, True, True]\n",
        "\n",
        "    get_dataset_array = [get_USairport500_dataset, get_OpenFlights_dataset]\n",
        "    title_array = [\"US_Airport500\", \"Open_Flights\"]\n",
        "    has_meta_array = [False, False]\n",
        "\n",
        "    f = open(\"./Similarities.txt\", 'w', encoding=\"utf-8\")\n",
        "    for i in range(len(get_dataset_array)):\n",
        "        get_dataset = get_dataset_array[i]\n",
        "        has_meta = has_meta_array[i]\n",
        "        title = title_array[i]\n",
        "        f.write(\"\\n\" + title + '\\n')\n",
        "\n",
        "        if title != \"Amazon\":\n",
        "            partition_Meta = None\n",
        "            clusters_Meta = -1\n",
        "            if has_meta:\n",
        "                D, D_ig, partition_Meta, B, B_ig, T, T_ig, S, S_ig = get_graphs(get_dataset, has_meta, True)\n",
        "                clusters_Meta = len(set(partition_Meta.values()))\n",
        "            else:\n",
        "                D, D_ig, B, B_ig, T, T_ig, S, S_ig = get_graphs(get_dataset, has_meta, True)\n",
        "        else:\n",
        "            D = nx.read_weighted_edgelist(\"drive/MyDrive/GSMB/D_Amazon\", nodetype=int)\n",
        "            B = nx.read_weighted_edgelist(\"drive/MyDrive/GSMB/B_Amazon\", nodetype=int)\n",
        "            T = nx.read_weighted_edgelist(\"drive/MyDrive/GSMB/T_Amazon\", nodetype=int)\n",
        "            S = nx.read_weighted_edgelist(\"drive/MyDrive/GSMB/S_Amazon\", nodetype=int)\n",
        "\n",
        "            for u, v, data in D.edges(data=True):\n",
        "                data[\"proximity\"] = data.pop(\"weight\")\n",
        "            for u, v, data in B.edges(data=True):\n",
        "                data[\"proximity\"] = data.pop(\"weight\")\n",
        "            for u, v, data in T.edges(data=True):\n",
        "                data[\"proximity\"] = data.pop(\"weight\")\n",
        "            for u, v, data in S.edges(data=True):\n",
        "                data[\"proximity\"] = data.pop(\"weight\")\n",
        "\n",
        "            partition_Meta = get_Amazon_meta()\n",
        "            clusters_Meta = len(set(partition_Meta.values()))\n",
        "\n",
        "        partitions_D = get_Bayesian_partition(D)\n",
        "        partitions_B = get_Bayesian_partition(B)\n",
        "        partitions_T = get_Bayesian_partition(T)\n",
        "        partitions_S = get_Bayesian_partition(S)\n",
        "\n",
        "        get_similarities(f, \"Original\", partitions_D, partitions_D, partition_Meta)\n",
        "        get_similarities(f, \"Backbone\", partitions_B, partitions_D, partition_Meta)\n",
        "        get_similarities(f, \"Threshold\", partitions_T, partitions_D, partition_Meta)\n",
        "        get_similarities(f, \"Spielman\", partitions_S, partitions_D, partition_Meta)\n",
        "\n",
        "    f.close()\n",
        "\n",
        "np.random.seed(11)\n",
        "compute_similarities_real_datasets_Bayesian()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}